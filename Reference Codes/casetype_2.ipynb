{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1aa396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/snowflake/connector/options.py:103: UserWarning: You have an incompatible version of 'pyarrow' installed (9.0.0), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "2023-09-12 12:44:12.592042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-12 12:44:13.398761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-12 12:44:13.398844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-12 12:44:13.398850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import snowflake.connector\n",
    "import joblib\n",
    "import scipy\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy.dialects import registry\n",
    "\n",
    "from azureml.core import Workspace, Dataset, Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "from azure.identity import ManagedIdentityCredential, InteractiveBrowserCredential, DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from sklearn.metrics import jaccard_score, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer, sample_dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bdaba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e767ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_row', 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2fa38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '75310040-ed56-4e53-83e0-80f23ef48cd6'\n",
    "resource_group = 'dlsap-PaaS-PRD-rgp-001'\n",
    "workspace_name = 'Finance_DS'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c47132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "secret_client = SecretClient(vault_url=\"https://financeds.vault.azure.net/\", credential=credential)\n",
    "        \n",
    "account = secret_client.get_secret(\"snowflake-account\").value\n",
    "user = secret_client.get_secret(\"snowflake-user\").value\n",
    "password = secret_client.get_secret(\"snowflake-password\").value\n",
    "database = secret_client.get_secret(\"snowflake-database\").value\n",
    "role = secret_client.get_secret(\"snowflake-role\").value\n",
    "warehouse = secret_client.get_secret(\"snowflake-warehouse\").value\n",
    "\n",
    "engine_1 = create_engine(URL(\n",
    "            account = account,\n",
    "            user = user,\n",
    "            password = password,\n",
    "            database = database,\n",
    "            role = role,\n",
    "            warehouse=warehouse,\n",
    "            schema='sinergi'\n",
    "        ))\n",
    "\n",
    "query_1 = f\"\"\" select * from RW_ACTUALS_ALL_TR\n",
    "        \"\"\"\n",
    "incidents = pd.read_sql(query_1, con=engine_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6425db99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseno</th>\n",
       "      <th>company</th>\n",
       "      <th>functional_group</th>\n",
       "      <th>function</th>\n",
       "      <th>functional_area</th>\n",
       "      <th>functional_location</th>\n",
       "      <th>functional_sub_location</th>\n",
       "      <th>location_sid</th>\n",
       "      <th>location_short</th>\n",
       "      <th>country_short</th>\n",
       "      <th>sl_country</th>\n",
       "      <th>sl_location_lvl_1</th>\n",
       "      <th>sl_location_lvl_2</th>\n",
       "      <th>sl_location_lvl_3</th>\n",
       "      <th>sl_location_lvl_4</th>\n",
       "      <th>title</th>\n",
       "      <th>employment_category</th>\n",
       "      <th>case_type</th>\n",
       "      <th>case_severity</th>\n",
       "      <th>actual_workdays_lta</th>\n",
       "      <th>case_description</th>\n",
       "      <th>status</th>\n",
       "      <th>hazard_physical_security_event</th>\n",
       "      <th>work_process</th>\n",
       "      <th>imm_action_taken_recom</th>\n",
       "      <th>full_investigation_done</th>\n",
       "      <th>case_occurence_date</th>\n",
       "      <th>created_date</th>\n",
       "      <th>modified_date</th>\n",
       "      <th>approved_within_deadline</th>\n",
       "      <th>personal_injuries</th>\n",
       "      <th>case_closed_date</th>\n",
       "      <th>cases_no_of_registrations</th>\n",
       "      <th>valid_from</th>\n",
       "      <th>valid_to</th>\n",
       "      <th>case_description_en</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32776</td>\n",
       "      <td>Uniper Group (70006651)</td>\n",
       "      <td>COO (70017898)</td>\n",
       "      <td>Asset Operations (70017927)</td>\n",
       "      <td>CCGT (70017930)</td>\n",
       "      <td>Grain UK (70019210)</td>\n",
       "      <td></td>\n",
       "      <td>8138</td>\n",
       "      <td>PP GRA</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>Gas Turbine Fleet</td>\n",
       "      <td>Grain</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>61BFT20 CB faulty</td>\n",
       "      <td>Uniper employee</td>\n",
       "      <td>Process Safety</td>\n",
       "      <td>Near Hits</td>\n",
       "      <td>NaN</td>\n",
       "      <td>415V circuit breaker from 11kv transformer sup...</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Occupational Safety and Process Safety</td>\n",
       "      <td>Inspection and monitoring</td>\n",
       "      <td>Recommended to fault find why this has happene...</td>\n",
       "      <td>Investigated</td>\n",
       "      <td>2023-03-19</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-05-25 01:50:46</td>\n",
       "      <td>2100-12-31</td>\n",
       "      <td>415V circuit breaker from 11kv transformer sup...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32024</td>\n",
       "      <td>Uniper Group (70006651)</td>\n",
       "      <td>COO (70017898)</td>\n",
       "      <td>Nuclear Sweden (70017948)</td>\n",
       "      <td>PM Oskarshamn (70018698)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8141</td>\n",
       "      <td>NPP OS</td>\n",
       "      <td>NO</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>Nuclear Sweden</td>\n",
       "      <td>Oskarshamn (Nuclear)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Oskarshamn 1 - RIO - Pardörr mellan 4.71 och 4...</td>\n",
       "      <td>Contractor</td>\n",
       "      <td>Safety</td>\n",
       "      <td>Observations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oskarshamn 1 - RIO - Pardörr mellan 4.71 och 4...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Occupational Safety and Process Safety</td>\n",
       "      <td>-- Not selected --</td>\n",
       "      <td>None</td>\n",
       "      <td>Not investigated</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>-- Not selected --</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-05-25 01:50:46</td>\n",
       "      <td>2100-12-31</td>\n",
       "      <td>Oskarshamn 1 - RIO - Pair door between 4.71 an...</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29905</td>\n",
       "      <td>Uniper Group (70006651)</td>\n",
       "      <td>COO (70017898)</td>\n",
       "      <td>Asset Operations (70017927)</td>\n",
       "      <td>Hydro DE (70018324)</td>\n",
       "      <td>Kraftwerksgruppe Lech (70018328)</td>\n",
       "      <td>Betrieb &amp; Instandhaltung (70018347)</td>\n",
       "      <td>8279</td>\n",
       "      <td>FIS</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Hydro DE</td>\n",
       "      <td>Rivergroup Lech</td>\n",
       "      <td>Finsterau KW</td>\n",
       "      <td>None</td>\n",
       "      <td>Fluchtwegekennzeichnung</td>\n",
       "      <td>Uniper employee</td>\n",
       "      <td>None</td>\n",
       "      <td>Observations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fluchtwegekennzeichnung Lagerraum Ausgangsbauw...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Lack of emergency routes</td>\n",
       "      <td>-- Not selected --</td>\n",
       "      <td>Kraftwerker eingewiesen und Anbringung angeord...</td>\n",
       "      <td>Not investigated</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-05-25 01:50:46</td>\n",
       "      <td>2100-12-31</td>\n",
       "      <td>Escape route marking storage room starting str...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caseno                  company functional_group  \\\n",
       "0   32776  Uniper Group (70006651)   COO (70017898)   \n",
       "1   32024  Uniper Group (70006651)   COO (70017898)   \n",
       "2   29905  Uniper Group (70006651)   COO (70017898)   \n",
       "\n",
       "                      function           functional_area  \\\n",
       "0  Asset Operations (70017927)           CCGT (70017930)   \n",
       "1    Nuclear Sweden (70017948)  PM Oskarshamn (70018698)   \n",
       "2  Asset Operations (70017927)       Hydro DE (70018324)   \n",
       "\n",
       "                functional_location              functional_sub_location  \\\n",
       "0               Grain UK (70019210)                                        \n",
       "1                                                                          \n",
       "2  Kraftwerksgruppe Lech (70018328)  Betrieb & Instandhaltung (70018347)   \n",
       "\n",
       "  location_sid location_short country_short sl_country  sl_location_lvl_1  \\\n",
       "0         8138         PP GRA            UK         UK  Gas Turbine Fleet   \n",
       "1         8141         NPP OS            NO     Sweden     Nuclear Sweden   \n",
       "2         8279            FIS            DE    Germany           Hydro DE   \n",
       "\n",
       "      sl_location_lvl_2 sl_location_lvl_3 sl_location_lvl_4  \\\n",
       "0                 Grain              None              None   \n",
       "1  Oskarshamn (Nuclear)              None              None   \n",
       "2       Rivergroup Lech      Finsterau KW              None   \n",
       "\n",
       "                                               title employment_category  \\\n",
       "0                                  61BFT20 CB faulty     Uniper employee   \n",
       "1  Oskarshamn 1 - RIO - Pardörr mellan 4.71 och 4...          Contractor   \n",
       "2                            Fluchtwegekennzeichnung     Uniper employee   \n",
       "\n",
       "        case_type case_severity  actual_workdays_lta  \\\n",
       "0  Process Safety     Near Hits                  NaN   \n",
       "1          Safety  Observations                  NaN   \n",
       "2            None  Observations                  NaN   \n",
       "\n",
       "                                    case_description    status  \\\n",
       "0  415V circuit breaker from 11kv transformer sup...  Approved   \n",
       "1  Oskarshamn 1 - RIO - Pardörr mellan 4.71 och 4...    Closed   \n",
       "2  Fluchtwegekennzeichnung Lagerraum Ausgangsbauw...    Closed   \n",
       "\n",
       "           hazard_physical_security_event               work_process  \\\n",
       "0  Occupational Safety and Process Safety  Inspection and monitoring   \n",
       "1  Occupational Safety and Process Safety         -- Not selected --   \n",
       "2                Lack of emergency routes         -- Not selected --   \n",
       "\n",
       "                              imm_action_taken_recom full_investigation_done  \\\n",
       "0  Recommended to fault find why this has happene...            Investigated   \n",
       "1                                               None        Not investigated   \n",
       "2  Kraftwerker eingewiesen und Anbringung angeord...        Not investigated   \n",
       "\n",
       "  case_occurence_date created_date modified_date approved_within_deadline  \\\n",
       "0          2023-03-19   2023-03-20    2023-05-24                       No   \n",
       "1          2023-02-15   2023-02-15    2023-05-24       -- Not selected --   \n",
       "2          2022-11-21   2022-11-25    2023-05-24                       No   \n",
       "\n",
       "   personal_injuries case_closed_date  cases_no_of_registrations  \\\n",
       "0                NaN             None                        1.0   \n",
       "1                NaN       2023-03-17                        1.0   \n",
       "2                NaN       2023-05-24                        1.0   \n",
       "\n",
       "           valid_from   valid_to  \\\n",
       "0 2023-05-25 01:50:46 2100-12-31   \n",
       "1 2023-05-25 01:50:46 2100-12-31   \n",
       "2 2023-05-25 01:50:46 2100-12-31   \n",
       "\n",
       "                                 case_description_en language  \n",
       "0  415V circuit breaker from 11kv transformer sup...       en  \n",
       "1  Oskarshamn 1 - RIO - Pair door between 4.71 an...       sv  \n",
       "2  Escape route marking storage room starting str...       de  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incidents.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26224ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents = incidents.drop_duplicates(['case_description_en'], keep=False)\n",
    "incidents = incidents[~incidents['case_type'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0271c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = incidents['case_description_en'].values\n",
    "targets = incidents['case_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c20d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, texts_test, targets_train, targets_test = train_test_split(texts, targets, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7be468ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "Y_train = enc.fit_transform(targets_train.reshape(-1, 1))\n",
    "Y_test = enc.fit_transform(targets_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaed1e8",
   "metadata": {},
   "source": [
    "## 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0da53d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SetFitModel.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61bcef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(targets_train.reshape(-1, 1))\n",
    "\n",
    "targets_test_ = ord_enc.transform(targets_test.reshape(-1, 1))\n",
    "targets_train_ = ord_enc.transform(targets_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5791a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_test_ = targets_test_.flatten()\n",
    "targets_train_ = targets_train_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fbebbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = Dataset.from_dict({'text':  texts_test, 'label': targets_test_})\n",
    "train_dataset = Dataset.from_dict({'text':  texts_train, 'label': targets_train_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76d83840",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric='accuracy',\n",
    "    num_iterations=5,\n",
    "    num_epochs=2,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "876a9bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638681ce44f94e298134231d349137e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Training Pairs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 74840\n",
      "  Num epochs = 2\n",
      "  Total optimization steps = 18710\n",
      "  Total train batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326b32d6635a4660916b1c96e4fd6ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906983ee0c51423ebe3f62ac25d1c690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/9355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bc0bb39072477dab4dbec319723699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/9355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60f496cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8039364118092355}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9648f3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data_.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model._save_pretrained('./model__/')\n",
    "joblib.dump((texts_train, texts_test, targets_train, targets_test), './data_.pkl', compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed599ab7",
   "metadata": {},
   "source": [
    "### logistic regression 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c7c7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = SentenceTransformer('all-mpnet-base-v2')\n",
    "X_train = model_.encode(texts_train, convert_to_tensor=False)\n",
    "X_test = model_.encode(texts_test, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18060e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000).fit(X_train, targets_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1adabeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a36b1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_back = ord_enc.inverse_transform(labels_pred.reshape(-1, 1)).flatten()\n",
    "targets_back = ord_enc.inverse_transform(targets_test_.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9aeb03f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "Asset and Reputation damage/loss       1.00      0.03      0.06        32\n",
      "                     Environment       0.64      0.47      0.54       102\n",
      "            Information Security       0.00      0.00      0.00         6\n",
      "                Operational loss       0.00      0.00      0.00        13\n",
      "                  Process Safety       0.68      0.48      0.56       196\n",
      "                          Safety       0.84      0.96      0.90       972\n",
      "\n",
      "                        accuracy                           0.81      1321\n",
      "                       macro avg       0.53      0.32      0.34      1321\n",
      "                    weighted avg       0.79      0.81      0.79      1321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets_back, preds_back))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11233224",
   "metadata": {},
   "source": [
    "### logistic regression 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21e0af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = model.model_body.encode(texts_train, convert_to_tensor=False)\n",
    "X_test = model.model_body.encode(texts_test, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cdf8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000).fit(X_train, targets_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2208c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9dd6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_back = ord_enc.inverse_transform(labels_pred.reshape(-1, 1)).flatten()\n",
    "targets_back = ord_enc.inverse_transform(targets_test_.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce2c0915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "Asset and Reputation damage/loss       0.11      0.25      0.16        32\n",
      "                     Environment       0.59      0.57      0.58       102\n",
      "            Information Security       1.00      0.33      0.50         6\n",
      "                Operational loss       0.00      0.00      0.00        13\n",
      "                  Process Safety       0.70      0.59      0.64       196\n",
      "                          Safety       0.89      0.90      0.90       972\n",
      "\n",
      "                        accuracy                           0.80      1321\n",
      "                       macro avg       0.55      0.44      0.46      1321\n",
      "                    weighted avg       0.81      0.80      0.81      1321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets_back, preds_back))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5f952",
   "metadata": {},
   "source": [
    "### distill-bert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ac4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18429677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ids, masks, y):\n",
    "        self.ids = ids\n",
    "        self.masks = masks\n",
    "        self.y = y\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index:int):\n",
    "        return self.ids[index], self.masks[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ids.shape[0]\n",
    "    \n",
    "    \n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 7)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1.last_hidden_state \n",
    "        \n",
    "        pooler = torch.mean(hidden_state, dim=1)\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352938af",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, texts_test, targets_train, targets_test = joblib.load('data_.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b1da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(targets_train.reshape(-1, 1))\n",
    "y_train = enc.transform(targets_train.reshape(-1, 1)).toarray()\n",
    "y_test = enc.transform(targets_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbe52794",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "tokens = tokenizer.batch_encode_plus(\n",
    "            list(texts_train),\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "            )\n",
    "\n",
    "train_ids = tokens['input_ids']\n",
    "train_masks = tokens['attention_mask']\n",
    "\n",
    "tokens = tokenizer.batch_encode_plus(\n",
    "            list(texts_test),\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "            )\n",
    "\n",
    "test_ids = tokens['input_ids']\n",
    "test_masks = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb579d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train)\n",
    "train_ids = torch.tensor(train_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "y_test = torch.tensor(y_test)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "467bc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_ids, train_masks, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f2dfe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(test_ids, test_masks, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c22dbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DistillBERTClass().to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e374aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "model.train()\n",
    "\n",
    "for iter_ in range(num_epochs):\n",
    "\n",
    "    losses = []\n",
    "    correct = 0\n",
    "\n",
    "    for ids, masks, y in train_loader:\n",
    "        ids = ids.to(device)\n",
    "        masks = masks.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_ = model(ids, masks)\n",
    "        loss = loss_function(y_, y)\n",
    "        \n",
    "        preds = torch.argmax(y_, dim=1)\n",
    "        true = torch.argmax(y, dim=1)\n",
    "        correct += (preds == true).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print('###############')\n",
    "    print(np.mean(losses))\n",
    "    print((correct / train_dataset.__len__()).item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f7c75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_list = []\n",
    "for ids, masks, y in test_loader:\n",
    "    ids = ids.to(device)\n",
    "    masks = masks.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_ = model(ids, masks)\n",
    "        \n",
    "    preds = torch.argmax(y_, dim=1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    pred_list.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8763b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(pred_list)\n",
    "preds = np.eye(7)[preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3c3e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_back = enc.inverse_transform(y_test.numpy()).flatten()\n",
    "y_pred_back = enc.inverse_transform(preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "faab81f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "Asset and Reputation damage/loss       0.00      0.00      0.00        32\n",
      "                     Environment       0.10      0.12      0.11       102\n",
      "            Information Security       0.00      0.00      0.00         6\n",
      "                Operational loss       0.00      0.00      0.00        13\n",
      "                  Process Safety       0.17      0.16      0.17       196\n",
      "                          Safety       0.74      0.77      0.76       972\n",
      "\n",
      "                        accuracy                           0.60      1321\n",
      "                       macro avg       0.17      0.18      0.17      1321\n",
      "                    weighted avg       0.58      0.60      0.59      1321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_back, y_pred_back))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e9436",
   "metadata": {},
   "source": [
    "### distillbert -- encode every sentence on it's own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3368d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c02d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, texts_test, targets_train, targets_test = joblib.load('data_.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66afbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(targets_train.reshape(-1, 1))\n",
    "y_train = enc.transform(targets_train.reshape(-1, 1)).toarray()\n",
    "y_test = enc.transform(targets_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf954a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_batch(sequence_data, batch_size, seq_len):\n",
    "    shp = tuple(sequence_data.shape)\n",
    "    batch_data = torch.reshape(sequence_data, [shp[0]*shp[1], *shp[2:]])\n",
    "    return batch_data\n",
    "\n",
    "def batch_to_seq(batch_data, batch_size, seq_len):\n",
    "    shp = tuple(batch_data.shape)\n",
    "    seq_data = torch.reshape(batch_data, [batch_size, seq_len, *shp[1:]])\n",
    "    return seq_data\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ids, masks, y):\n",
    "        self.ids = ids\n",
    "        self.masks = masks\n",
    "        self.y = y\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index:int):\n",
    "        return self.ids[index], self.masks[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ids.shape[0]\n",
    "\n",
    "class SequencesBert(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 7)\n",
    "       \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        seq_len = input_ids.shape[1]\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        input_ids = seq_to_batch(input_ids, batch_size, seq_len)\n",
    "        attention_mask = seq_to_batch(attention_mask, batch_size, seq_len)\n",
    "        \n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1.last_hidden_state\n",
    "        hidden_state = batch_to_seq(hidden_state, batch_size, seq_len)\n",
    "        hidden_state = torch.mean(hidden_state, dim=[1, 2])\n",
    "        \n",
    "        \n",
    "        pooler = self.pre_classifier(hidden_state)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "multiple_dots = r'\\.{2,}'\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    if sentences and not sentences[-1]: sentences = sentences[:-1]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def create_sequences(tokens, sentences_max, max_len):\n",
    "    ids = tokens['input_ids']\n",
    "    masks = tokens['attention_mask']\n",
    "    number_missing = sentences_max - len(ids) \n",
    "    missing_sequences = [[0] * max_len for i in range(number_missing)]\n",
    "    ids = ids + missing_sequences\n",
    "    masks = masks + missing_sequences\n",
    "    \n",
    "    return ids, masks\n",
    "        \n",
    "    \n",
    "def pad_empty_sentences(sentences, sentences_max):\n",
    "    number_missing = sentences_max - len(sentences) \n",
    "    sentences = sentences + ['[PAD]'] * number_missing\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e691b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train_segmented = [split_into_sentences(x) for x in texts_train]\n",
    "texts_test_segmented = [split_into_sentences(x) for x in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378ff072",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_train = [len(x) for x in texts_train_segmented]\n",
    "lens_test = [len(x) for x in texts_test_segmented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a25f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_max = max(max(lens_train), max(lens_test))\n",
    "all_sentences = sum(texts_train_segmented, []) + sum(texts_test_segmented, [])\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "all_lens = [len(tokenizer.encode(x)) for x in all_sentences]\n",
    "max_len = max(all_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24de7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train_segmented = [pad_empty_sentences(x, sentences_max) for x in texts_train_segmented]\n",
    "texts_test_segmented = [pad_empty_sentences(x, sentences_max) for x in texts_test_segmented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9112cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ = [tokenizer.batch_encode_plus(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "            ) for x in texts_train_segmented]\n",
    "\n",
    "ids_train = np.stack([x['input_ids'] for x in tokens_])\n",
    "masks_train = np.stack([x['attention_mask'] for x in tokens_])\n",
    "\n",
    "tokens_ = [tokenizer.batch_encode_plus(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "            ) for x in texts_test_segmented]\n",
    "\n",
    "ids_test = np.stack([x['input_ids'] for x in tokens_])\n",
    "masks_test = np.stack([x['attention_mask'] for x in tokens_])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c95e9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = torch.tensor(ids_train)\n",
    "masks_train = torch.tensor(masks_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "ids_test = torch.tensor(ids_test)\n",
    "masks_test = torch.tensor(masks_test)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22a8cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(ids_train, masks_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "test_dataset = TextDataset(ids_test, masks_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6f0ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SequencesBert().to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04ab86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "0.9000213970094202\n",
      "###############\n",
      "0.8801893828788157\n",
      "###############\n",
      "0.8511827472400245\n",
      "###############\n",
      "0.8072080363503064\n",
      "###############\n",
      "0.7677551972881888\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "model.train()\n",
    "\n",
    "for iter_ in range(num_epochs):\n",
    "\n",
    "    losses = []\n",
    "    correct = 0\n",
    "\n",
    "    for ids, masks, y in train_loader:\n",
    "        ids = ids.to(device)\n",
    "        masks = masks.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_ = model(ids, masks)\n",
    "        \n",
    "        loss = loss_function(y_, y)\n",
    "        preds = torch.argmax(y_, dim=1)\n",
    "        true = torch.argmax(y, dim=1)\n",
    "        correct += (preds == true).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print('###############')\n",
    "    print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78f7166a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_state_dict.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "model = model.to('cpu')\n",
    "joblib.dump(model, 'model.pkl')\n",
    "joblib.dump(model.state_dict(), 'model_state_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd5e09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3751799e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_list = []\n",
    "for ids, masks, y in test_loader:\n",
    "    ids = ids.to(device)\n",
    "    masks = masks.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_ = model(ids, masks)\n",
    "        \n",
    "    preds = torch.argmax(y_, dim=1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    pred_list.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69c55137",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(pred_list)\n",
    "preds = np.eye(7)[preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73409319",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_back = enc.inverse_transform(y_test.numpy()).flatten()\n",
    "y_pred_back = enc.inverse_transform(preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65c70c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "Asset and Reputation damage/loss       0.00      0.00      0.00        32\n",
      "                     Environment       0.00      0.00      0.00       102\n",
      "            Information Security       0.00      0.00      0.00         6\n",
      "                Operational loss       0.00      0.00      0.00        13\n",
      "                  Process Safety       0.11      0.06      0.07       196\n",
      "                          Safety       0.74      0.92      0.82       972\n",
      "\n",
      "                        accuracy                           0.69      1321\n",
      "                       macro avg       0.14      0.16      0.15      1321\n",
      "                    weighted avg       0.56      0.69      0.61      1321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_back, y_pred_back))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfe0eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': texts_test, 'label': y_test_back, 'label_pred': y_pred_back})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "508bcfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distill-bert-results.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df, 'distill-bert-results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9987b5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Process Safety', 'Safety'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['label_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2138826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8b88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b47c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
